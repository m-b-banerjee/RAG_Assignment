# RAG_Assignment


Legal firms and departments often handle large volumes of complex documents, making it difficult to efficiently find specific information. A streamlined RAG-based solution provides quicker and more precise responses to legal queries by dynamically retrieving and contextualising relevant excerpts from a legal corpus.


By adopting this approach, legal professionals can save time on document review, enhance research accuracy, and make more informed decisions. By adopting this approach, legal professionals can save time on document review, enhance research accuracy, and make more informed decisions.

 

In the next segment, we will have a walkthrough of the assignment tasks.
<br/>
<br/>

**Overview of the Task**

This assignment aimed to implement a complete Retrieval-Augmented Generation (RAG) pipeline using LangChain, tailored for answering legal-domain questions. The pipeline included:

- Created Document length distribution, Boxplot for Word Category

- Performed Word Frequency Analysis and 'Company' is the most common word.

- Drawn the pictorial representation for the most common word.

- Created 2 Heatmaps to see the similarities between First 10 documents and Random 10 documents

- Text preprocessing and chunking

- Vector database creation using LangChain

- Retrieval and answer generation using a query engine

- Evaluation using BLEU, ROUGE, and RAGAS dataset metrics


<br/>
<br/>


 Here's a summary of the key outcomes and insights:

<br/>
<br/>


#### **Final Evaluation Scores Results for RAG - (Top 100 Questions)**

**Metric - 	Score	 -    Detailed Interpretation**


- Average BLEU score	-  0.7376	 - Comparatively good lexical match between generated and ground-truth answers
- ROUGE-1 F1 - 	0.823 - Good token-level overlap with ground truth
- Faithfulness - 	Mostly 1 - 	Most answers are grounded in retrieved context
- Answer Relevance - 	0.84 - 	Answers are contextually relevant to the questions
- Context Precision - 	Mostly 1	- Retrieved chunks almost matched and contributed to useful information
- Context Recall	- Mostly 1 - 	Most relevant content was retrieved and Analyzed.


<br/>
<br/>


**Insights from the Results**

**High Faithfulness and Context Scores:** The answers generated by the Model is grounded in its retrieved context, indicating that the vector retrieval or chunking process might show proper and revent answers in read world scenario.

**Mostly Accurate Answer Relevance:** The model generated answers are mostly inclined with the question intent. Hence, the model is likely to be relyed on general purpose use for Legal documentations.

**BLEU and ROUGE Scores are High:** This reflects a strong lexical and token-level distinguishing with ground truth, highlighting the model's efficiency in replicating precise legal responses.



<br/>
<br/>


**Conclusion**

This project demonstrates how a RAG pipeline can be constructed for legal documentation using OpenAI, Langchain, nltk, ragas and evaluated using both traditional and semantic metrics. The current configuration performs well with retrieval quality and grounding, which are crucial in high-stakes domains like law.
